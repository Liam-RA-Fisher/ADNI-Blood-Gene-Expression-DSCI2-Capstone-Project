{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd5e153",
   "metadata": {},
   "source": [
    "# Capstone Project Modelling\n",
    "\n",
    "Liam Fisher\n",
    "\n",
    "3/1/2022\n",
    "\n",
    "### For context and preperation for this portion of the project see:\n",
    "\n",
    "Capstone_Project_Pt1.html, Capstone_Project_Pt1.Rmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c44d1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import kerastuner.tuners as kt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7389ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "train = pd.read_csv(\"Data/train_set.csv\") \n",
    "test = pd.read_csv(\"Data/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7911976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0              int64\n",
      "RID                     int64\n",
      "baseline_diagnosis     object\n",
      "study_enrolled         object\n",
      "VISCODE2               object\n",
      "11715100_at           float64\n",
      "11715101_s_at         float64\n",
      "11715102_x_at         float64\n",
      "dtype: object\n",
      "Unnamed: 0              int64\n",
      "RID                     int64\n",
      "baseline_diagnosis     object\n",
      "study_enrolled         object\n",
      "VISCODE2               object\n",
      "11715100_at           float64\n",
      "11715101_s_at         float64\n",
      "11715102_x_at         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Checking Data Types\n",
    "print(train.iloc[: , :8].dtypes)\n",
    "print(test.iloc[: , :8].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bce3076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       CN\n",
      "1       CN\n",
      "2       CN\n",
      "3       CN\n",
      "4       CN\n",
      "      ... \n",
      "553     CN\n",
      "554    MCI\n",
      "555     AD\n",
      "556    MCI\n",
      "557    MCI\n",
      "Name: baseline_diagnosis, Length: 558, dtype: object\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "553    1\n",
      "554    2\n",
      "555    0\n",
      "556    2\n",
      "557    2\n",
      "Length: 558, dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# Splitting Train and Test Into Features and Target\n",
    "X_train = train.iloc[ : , 5: ]\n",
    "y_train = train[\"baseline_diagnosis\"]\n",
    "X_test = test.iloc[ : , 5: ]\n",
    "y_test = test[\"baseline_diagnosis\"]\n",
    "y_train_num = pd.DataFrame(y_train)[\"baseline_diagnosis\"].astype('category').cat.codes\n",
    "y_test_num = pd.DataFrame(y_test)[\"baseline_diagnosis\"].astype('category').cat.codes\n",
    "print(y_train)\n",
    "print(y_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca8f8f",
   "metadata": {},
   "source": [
    "For the numerical encoding of the labels:\n",
    "\n",
    "0 = AD\n",
    "\n",
    "1 = CN\n",
    "\n",
    "2 = MCI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f135bc",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Due to the fact that the data contains 49390 variables feature extraction will be necassary. The two tequenique that will be implemented are PCA and a Variational Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c763e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49376</th>\n",
       "      <th>49377</th>\n",
       "      <th>49378</th>\n",
       "      <th>49379</th>\n",
       "      <th>49380</th>\n",
       "      <th>49381</th>\n",
       "      <th>49382</th>\n",
       "      <th>49383</th>\n",
       "      <th>49384</th>\n",
       "      <th>49385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.459237</td>\n",
       "      <td>-0.977341</td>\n",
       "      <td>-1.927645</td>\n",
       "      <td>-1.010870</td>\n",
       "      <td>0.971885</td>\n",
       "      <td>0.621014</td>\n",
       "      <td>-0.331472</td>\n",
       "      <td>-1.400078</td>\n",
       "      <td>-0.460223</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511882</td>\n",
       "      <td>0.632463</td>\n",
       "      <td>-2.328265</td>\n",
       "      <td>-1.474224</td>\n",
       "      <td>-0.265241</td>\n",
       "      <td>0.040190</td>\n",
       "      <td>1.321942</td>\n",
       "      <td>0.826098</td>\n",
       "      <td>0.853084</td>\n",
       "      <td>-0.502369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.454250</td>\n",
       "      <td>1.338511</td>\n",
       "      <td>-0.667945</td>\n",
       "      <td>1.796182</td>\n",
       "      <td>2.114344</td>\n",
       "      <td>-0.372569</td>\n",
       "      <td>1.240742</td>\n",
       "      <td>2.279812</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>1.680331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259085</td>\n",
       "      <td>2.542020</td>\n",
       "      <td>-0.289828</td>\n",
       "      <td>0.739156</td>\n",
       "      <td>-0.096633</td>\n",
       "      <td>-0.639389</td>\n",
       "      <td>-0.425508</td>\n",
       "      <td>1.968423</td>\n",
       "      <td>1.559449</td>\n",
       "      <td>2.903585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.065299</td>\n",
       "      <td>-0.465182</td>\n",
       "      <td>0.100303</td>\n",
       "      <td>-1.007701</td>\n",
       "      <td>-1.075800</td>\n",
       "      <td>-0.096574</td>\n",
       "      <td>2.350540</td>\n",
       "      <td>-0.640671</td>\n",
       "      <td>1.277619</td>\n",
       "      <td>2.166503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626789</td>\n",
       "      <td>-0.080813</td>\n",
       "      <td>-1.262718</td>\n",
       "      <td>0.334494</td>\n",
       "      <td>-0.932930</td>\n",
       "      <td>-1.149073</td>\n",
       "      <td>-1.198250</td>\n",
       "      <td>0.611518</td>\n",
       "      <td>0.922185</td>\n",
       "      <td>-1.167982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.110178</td>\n",
       "      <td>-0.095536</td>\n",
       "      <td>-2.311769</td>\n",
       "      <td>-0.541972</td>\n",
       "      <td>0.322619</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>-0.264678</td>\n",
       "      <td>-1.682835</td>\n",
       "      <td>-0.578396</td>\n",
       "      <td>-1.438046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.606553</td>\n",
       "      <td>-0.322316</td>\n",
       "      <td>-1.254997</td>\n",
       "      <td>-1.161531</td>\n",
       "      <td>-1.142005</td>\n",
       "      <td>-0.898702</td>\n",
       "      <td>-0.653818</td>\n",
       "      <td>-0.404583</td>\n",
       "      <td>-1.358146</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.077570</td>\n",
       "      <td>0.318645</td>\n",
       "      <td>-0.667945</td>\n",
       "      <td>-2.420732</td>\n",
       "      <td>-0.632551</td>\n",
       "      <td>-1.800845</td>\n",
       "      <td>-0.660301</td>\n",
       "      <td>-1.387959</td>\n",
       "      <td>0.429552</td>\n",
       "      <td>-0.863479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997240</td>\n",
       "      <td>-1.058057</td>\n",
       "      <td>0.034469</td>\n",
       "      <td>-1.345468</td>\n",
       "      <td>-1.445500</td>\n",
       "      <td>0.156434</td>\n",
       "      <td>-0.249885</td>\n",
       "      <td>-0.492940</td>\n",
       "      <td>-2.233424</td>\n",
       "      <td>-0.495288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>-2.164639</td>\n",
       "      <td>-0.353843</td>\n",
       "      <td>-0.504127</td>\n",
       "      <td>0.139198</td>\n",
       "      <td>-0.257974</td>\n",
       "      <td>-1.759446</td>\n",
       "      <td>0.208013</td>\n",
       "      <td>-0.664908</td>\n",
       "      <td>0.019421</td>\n",
       "      <td>0.958439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634450</td>\n",
       "      <td>-0.575051</td>\n",
       "      <td>0.351045</td>\n",
       "      <td>0.659450</td>\n",
       "      <td>-0.575481</td>\n",
       "      <td>-0.505261</td>\n",
       "      <td>0.285765</td>\n",
       "      <td>-1.313394</td>\n",
       "      <td>0.730238</td>\n",
       "      <td>0.163243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1.585251</td>\n",
       "      <td>0.968865</td>\n",
       "      <td>1.732830</td>\n",
       "      <td>0.278600</td>\n",
       "      <td>-0.033229</td>\n",
       "      <td>-0.034475</td>\n",
       "      <td>1.209914</td>\n",
       "      <td>0.845826</td>\n",
       "      <td>1.048224</td>\n",
       "      <td>0.624503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075232</td>\n",
       "      <td>-0.586284</td>\n",
       "      <td>2.003415</td>\n",
       "      <td>0.346756</td>\n",
       "      <td>3.659963</td>\n",
       "      <td>-1.149073</td>\n",
       "      <td>-0.021575</td>\n",
       "      <td>-1.995002</td>\n",
       "      <td>2.066189</td>\n",
       "      <td>-0.112915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0.174056</td>\n",
       "      <td>-0.246957</td>\n",
       "      <td>0.286716</td>\n",
       "      <td>-1.584319</td>\n",
       "      <td>-0.420291</td>\n",
       "      <td>0.897010</td>\n",
       "      <td>-1.235751</td>\n",
       "      <td>-0.992099</td>\n",
       "      <td>-0.182168</td>\n",
       "      <td>0.511553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001373</td>\n",
       "      <td>0.031514</td>\n",
       "      <td>-0.204893</td>\n",
       "      <td>-0.039512</td>\n",
       "      <td>-0.157332</td>\n",
       "      <td>-1.873362</td>\n",
       "      <td>-0.838223</td>\n",
       "      <td>0.056133</td>\n",
       "      <td>0.714882</td>\n",
       "      <td>-0.410316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0.014486</td>\n",
       "      <td>1.102472</td>\n",
       "      <td>0.196334</td>\n",
       "      <td>-1.270665</td>\n",
       "      <td>-0.557636</td>\n",
       "      <td>-1.172955</td>\n",
       "      <td>1.913814</td>\n",
       "      <td>0.595384</td>\n",
       "      <td>1.778118</td>\n",
       "      <td>0.604859</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.717328</td>\n",
       "      <td>-0.288618</td>\n",
       "      <td>2.211891</td>\n",
       "      <td>-1.290287</td>\n",
       "      <td>0.726177</td>\n",
       "      <td>0.558816</td>\n",
       "      <td>-1.013846</td>\n",
       "      <td>-1.395439</td>\n",
       "      <td>-0.367699</td>\n",
       "      <td>-0.693556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1.665036</td>\n",
       "      <td>0.474520</td>\n",
       "      <td>1.913594</td>\n",
       "      <td>-0.491280</td>\n",
       "      <td>-1.038342</td>\n",
       "      <td>0.455417</td>\n",
       "      <td>-1.682753</td>\n",
       "      <td>-0.018604</td>\n",
       "      <td>0.241865</td>\n",
       "      <td>-0.991160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619129</td>\n",
       "      <td>-0.917648</td>\n",
       "      <td>-0.567796</td>\n",
       "      <td>-1.363862</td>\n",
       "      <td>0.658733</td>\n",
       "      <td>1.658661</td>\n",
       "      <td>1.567814</td>\n",
       "      <td>-0.278359</td>\n",
       "      <td>1.482670</td>\n",
       "      <td>0.595184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 49386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6      \\\n",
       "0   -0.459237 -0.977341 -1.927645 -1.010870  0.971885  0.621014 -0.331472   \n",
       "1   -0.454250  1.338511 -0.667945  1.796182  2.114344 -0.372569  1.240742   \n",
       "2   -0.065299 -0.465182  0.100303 -1.007701 -1.075800 -0.096574  2.350540   \n",
       "3   -0.110178 -0.095536 -2.311769 -0.541972  0.322619  0.027624 -0.264678   \n",
       "4   -1.077570  0.318645 -0.667945 -2.420732 -0.632551 -1.800845 -0.660301   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "553 -2.164639 -0.353843 -0.504127  0.139198 -0.257974 -1.759446  0.208013   \n",
       "554  1.585251  0.968865  1.732830  0.278600 -0.033229 -0.034475  1.209914   \n",
       "555  0.174056 -0.246957  0.286716 -1.584319 -0.420291  0.897010 -1.235751   \n",
       "556  0.014486  1.102472  0.196334 -1.270665 -0.557636 -1.172955  1.913814   \n",
       "557  1.665036  0.474520  1.913594 -0.491280 -1.038342  0.455417 -1.682753   \n",
       "\n",
       "        7         8         9      ...     49376     49377     49378  \\\n",
       "0   -1.400078 -0.460223  0.197260  ...  0.511882  0.632463 -2.328265   \n",
       "1    2.279812  0.005519  1.680331  ...  0.259085  2.542020 -0.289828   \n",
       "2   -0.640671  1.277619  2.166503  ...  0.626789 -0.080813 -1.262718   \n",
       "3   -1.682835 -0.578396 -1.438046  ... -0.606553 -0.322316 -1.254997   \n",
       "4   -1.387959  0.429552 -0.863479  ... -0.997240 -1.058057  0.034469   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "553 -0.664908  0.019421  0.958439  ...  0.634450 -0.575051  0.351045   \n",
       "554  0.845826  1.048224  0.624503  ...  0.075232 -0.586284  2.003415   \n",
       "555 -0.992099 -0.182168  0.511553  ... -0.001373  0.031514 -0.204893   \n",
       "556  0.595384  1.778118  0.604859  ... -1.717328 -0.288618  2.211891   \n",
       "557 -0.018604  0.241865 -0.991160  ...  0.619129 -0.917648 -0.567796   \n",
       "\n",
       "        49379     49380     49381     49382     49383     49384     49385  \n",
       "0   -1.474224 -0.265241  0.040190  1.321942  0.826098  0.853084 -0.502369  \n",
       "1    0.739156 -0.096633 -0.639389 -0.425508  1.968423  1.559449  2.903585  \n",
       "2    0.334494 -0.932930 -1.149073 -1.198250  0.611518  0.922185 -1.167982  \n",
       "3   -1.161531 -1.142005 -0.898702 -0.653818 -0.404583 -1.358146  0.142000  \n",
       "4   -1.345468 -1.445500  0.156434 -0.249885 -0.492940 -2.233424 -0.495288  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "553  0.659450 -0.575481 -0.505261  0.285765 -1.313394  0.730238  0.163243  \n",
       "554  0.346756  3.659963 -1.149073 -0.021575 -1.995002  2.066189 -0.112915  \n",
       "555 -0.039512 -0.157332 -1.873362 -0.838223  0.056133  0.714882 -0.410316  \n",
       "556 -1.290287  0.726177  0.558816 -1.013846 -1.395439 -0.367699 -0.693556  \n",
       "557 -1.363862  0.658733  1.658661  1.567814 -0.278359  1.482670  0.595184  \n",
       "\n",
       "[558 rows x 49386 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling Training Data\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X_train)\n",
    "X_train_scaled_standard = pd.DataFrame(standard_scaler.transform(X_train))\n",
    "X_train_scaled_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9843dc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49376</th>\n",
       "      <th>49377</th>\n",
       "      <th>49378</th>\n",
       "      <th>49379</th>\n",
       "      <th>49380</th>\n",
       "      <th>49381</th>\n",
       "      <th>49382</th>\n",
       "      <th>49383</th>\n",
       "      <th>49384</th>\n",
       "      <th>49385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019473</td>\n",
       "      <td>0.367634</td>\n",
       "      <td>0.015570</td>\n",
       "      <td>0.573245</td>\n",
       "      <td>1.078015</td>\n",
       "      <td>-0.945259</td>\n",
       "      <td>-0.650025</td>\n",
       "      <td>0.748881</td>\n",
       "      <td>-0.613153</td>\n",
       "      <td>0.084311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667837</td>\n",
       "      <td>0.756022</td>\n",
       "      <td>0.420537</td>\n",
       "      <td>-0.235712</td>\n",
       "      <td>0.071976</td>\n",
       "      <td>-1.676641</td>\n",
       "      <td>0.786291</td>\n",
       "      <td>0.725119</td>\n",
       "      <td>-1.120131</td>\n",
       "      <td>-1.019281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.384438</td>\n",
       "      <td>1.160368</td>\n",
       "      <td>0.941986</td>\n",
       "      <td>0.687301</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.372618</td>\n",
       "      <td>-0.233851</td>\n",
       "      <td>0.664053</td>\n",
       "      <td>1.583480</td>\n",
       "      <td>1.194159</td>\n",
       "      <td>...</td>\n",
       "      <td>2.541917</td>\n",
       "      <td>1.115468</td>\n",
       "      <td>-0.583239</td>\n",
       "      <td>1.897962</td>\n",
       "      <td>1.319678</td>\n",
       "      <td>0.424689</td>\n",
       "      <td>1.181443</td>\n",
       "      <td>-0.707520</td>\n",
       "      <td>1.874242</td>\n",
       "      <td>-0.169563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.912066</td>\n",
       "      <td>-0.710128</td>\n",
       "      <td>-2.232685</td>\n",
       "      <td>-0.107924</td>\n",
       "      <td>-0.314161</td>\n",
       "      <td>-0.455368</td>\n",
       "      <td>0.644739</td>\n",
       "      <td>-1.537417</td>\n",
       "      <td>-1.329144</td>\n",
       "      <td>0.040114</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.472191</td>\n",
       "      <td>-1.810647</td>\n",
       "      <td>-1.579294</td>\n",
       "      <td>-0.885625</td>\n",
       "      <td>0.678966</td>\n",
       "      <td>-1.721350</td>\n",
       "      <td>-1.259718</td>\n",
       "      <td>-0.631786</td>\n",
       "      <td>-1.327434</td>\n",
       "      <td>-0.913067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.056676</td>\n",
       "      <td>-0.999609</td>\n",
       "      <td>0.151143</td>\n",
       "      <td>-0.497616</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>0.238841</td>\n",
       "      <td>-0.737617</td>\n",
       "      <td>0.019421</td>\n",
       "      <td>-1.128663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879586</td>\n",
       "      <td>0.211237</td>\n",
       "      <td>-1.170062</td>\n",
       "      <td>0.211869</td>\n",
       "      <td>-1.378057</td>\n",
       "      <td>-0.773516</td>\n",
       "      <td>-0.451852</td>\n",
       "      <td>0.232847</td>\n",
       "      <td>-0.536612</td>\n",
       "      <td>-1.231711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.773390</td>\n",
       "      <td>-1.271276</td>\n",
       "      <td>0.704733</td>\n",
       "      <td>0.861554</td>\n",
       "      <td>-0.033229</td>\n",
       "      <td>0.082823</td>\n",
       "      <td>0.911913</td>\n",
       "      <td>-0.188259</td>\n",
       "      <td>1.145543</td>\n",
       "      <td>0.752184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001373</td>\n",
       "      <td>0.621230</td>\n",
       "      <td>1.354821</td>\n",
       "      <td>-0.591325</td>\n",
       "      <td>-0.015701</td>\n",
       "      <td>-1.506746</td>\n",
       "      <td>0.101361</td>\n",
       "      <td>-0.126891</td>\n",
       "      <td>1.137166</td>\n",
       "      <td>-0.155401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2.891729</td>\n",
       "      <td>4.763299</td>\n",
       "      <td>2.320314</td>\n",
       "      <td>2.040136</td>\n",
       "      <td>1.895841</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>-1.194648</td>\n",
       "      <td>2.526215</td>\n",
       "      <td>0.763218</td>\n",
       "      <td>-0.377307</td>\n",
       "      <td>...</td>\n",
       "      <td>3.729297</td>\n",
       "      <td>1.980385</td>\n",
       "      <td>3.022633</td>\n",
       "      <td>2.327149</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>1.712312</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>0.422182</td>\n",
       "      <td>2.127613</td>\n",
       "      <td>0.545617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.253841</td>\n",
       "      <td>-1.689911</td>\n",
       "      <td>-0.803518</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.054173</td>\n",
       "      <td>0.151822</td>\n",
       "      <td>0.403256</td>\n",
       "      <td>-0.624514</td>\n",
       "      <td>0.088935</td>\n",
       "      <td>-0.225071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.461004</td>\n",
       "      <td>0.520136</td>\n",
       "      <td>-0.467419</td>\n",
       "      <td>-1.314812</td>\n",
       "      <td>0.564312</td>\n",
       "      <td>0.075957</td>\n",
       "      <td>-0.732849</td>\n",
       "      <td>0.731430</td>\n",
       "      <td>-1.811141</td>\n",
       "      <td>0.014543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-0.783363</td>\n",
       "      <td>-0.888270</td>\n",
       "      <td>-0.136950</td>\n",
       "      <td>-0.225149</td>\n",
       "      <td>-0.257974</td>\n",
       "      <td>-1.069457</td>\n",
       "      <td>-1.667339</td>\n",
       "      <td>-0.975941</td>\n",
       "      <td>-1.210971</td>\n",
       "      <td>0.437891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.805727</td>\n",
       "      <td>-1.097371</td>\n",
       "      <td>0.652178</td>\n",
       "      <td>-0.094694</td>\n",
       "      <td>-1.067817</td>\n",
       "      <td>0.317387</td>\n",
       "      <td>-0.302572</td>\n",
       "      <td>0.182357</td>\n",
       "      <td>-1.074064</td>\n",
       "      <td>-0.686475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.513142</td>\n",
       "      <td>-0.211329</td>\n",
       "      <td>1.461684</td>\n",
       "      <td>1.153031</td>\n",
       "      <td>-0.076929</td>\n",
       "      <td>-0.545066</td>\n",
       "      <td>1.261294</td>\n",
       "      <td>0.348981</td>\n",
       "      <td>-2.121601</td>\n",
       "      <td>0.555751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588487</td>\n",
       "      <td>2.929548</td>\n",
       "      <td>1.215836</td>\n",
       "      <td>0.168950</td>\n",
       "      <td>-0.575481</td>\n",
       "      <td>0.388922</td>\n",
       "      <td>0.768729</td>\n",
       "      <td>2.403895</td>\n",
       "      <td>-0.636425</td>\n",
       "      <td>0.694317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-0.743471</td>\n",
       "      <td>-0.322668</td>\n",
       "      <td>-1.306268</td>\n",
       "      <td>0.079001</td>\n",
       "      <td>-0.326647</td>\n",
       "      <td>-0.717563</td>\n",
       "      <td>0.742360</td>\n",
       "      <td>-0.628553</td>\n",
       "      <td>-0.314244</td>\n",
       "      <td>-0.013905</td>\n",
       "      <td>...</td>\n",
       "      <td>1.132383</td>\n",
       "      <td>0.660545</td>\n",
       "      <td>-0.961585</td>\n",
       "      <td>-0.634243</td>\n",
       "      <td>-0.481060</td>\n",
       "      <td>-0.165472</td>\n",
       "      <td>0.724823</td>\n",
       "      <td>-0.549740</td>\n",
       "      <td>-0.137363</td>\n",
       "      <td>-0.693556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 49386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6      \\\n",
       "0    0.019473  0.367634  0.015570  0.573245  1.078015 -0.945259 -0.650025   \n",
       "1   -0.384438  1.160368  0.941986  0.687301  0.228975  0.372618 -0.233851   \n",
       "2    0.912066 -0.710128 -2.232685 -0.107924 -0.314161 -0.455368  0.644739   \n",
       "3    1.056676 -0.999609  0.151143 -0.497616  0.228975  0.027624  0.238841   \n",
       "4   -0.773390 -1.271276  0.704733  0.861554 -0.033229  0.082823  0.911913   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "181  2.891729  4.763299  2.320314  2.040136  1.895841  0.110423 -1.194648   \n",
       "182  0.253841 -1.689911 -0.803518  0.009300  0.054173  0.151822  0.403256   \n",
       "183 -0.783363 -0.888270 -0.136950 -0.225149 -0.257974 -1.069457 -1.667339   \n",
       "184  0.513142 -0.211329  1.461684  1.153031 -0.076929 -0.545066  1.261294   \n",
       "185 -0.743471 -0.322668 -1.306268  0.079001 -0.326647 -0.717563  0.742360   \n",
       "\n",
       "        7         8         9      ...     49376     49377     49378  \\\n",
       "0    0.748881 -0.613153  0.084311  ... -0.667837  0.756022  0.420537   \n",
       "1    0.664053  1.583480  1.194159  ...  2.541917  1.115468 -0.583239   \n",
       "2   -1.537417 -1.329144  0.040114  ... -1.472191 -1.810647 -1.579294   \n",
       "3   -0.737617  0.019421 -1.128663  ...  0.879586  0.211237 -1.170062   \n",
       "4   -0.188259  1.145543  0.752184  ... -0.001373  0.621230  1.354821   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "181  2.526215  0.763218 -0.377307  ...  3.729297  1.980385  3.022633   \n",
       "182 -0.624514  0.088935 -0.225071  ... -0.461004  0.520136 -0.467419   \n",
       "183 -0.975941 -1.210971  0.437891  ... -0.805727 -1.097371  0.652178   \n",
       "184  0.348981 -2.121601  0.555751  ...  0.588487  2.929548  1.215836   \n",
       "185 -0.628553 -0.314244 -0.013905  ...  1.132383  0.660545 -0.961585   \n",
       "\n",
       "        49379     49380     49381     49382     49383     49384     49385  \n",
       "0   -0.235712  0.071976 -1.676641  0.786291  0.725119 -1.120131 -1.019281  \n",
       "1    1.897962  1.319678  0.424689  1.181443 -0.707520  1.874242 -0.169563  \n",
       "2   -0.885625  0.678966 -1.721350 -1.259718 -0.631786 -1.327434 -0.913067  \n",
       "3    0.211869 -1.378057 -0.773516 -0.451852  0.232847 -0.536612 -1.231711  \n",
       "4   -0.591325 -0.015701 -1.506746  0.101361 -0.126891  1.137166 -0.155401  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "181  2.327149  0.078720  1.712312  0.408701  0.422182  2.127613  0.545617  \n",
       "182 -1.314812  0.564312  0.075957 -0.732849  0.731430 -1.811141  0.014543  \n",
       "183 -0.094694 -1.067817  0.317387 -0.302572  0.182357 -1.074064 -0.686475  \n",
       "184  0.168950 -0.575481  0.388922  0.768729  2.403895 -0.636425  0.694317  \n",
       "185 -0.634243 -0.481060 -0.165472  0.724823 -0.549740 -0.137363 -0.693556  \n",
       "\n",
       "[186 rows x 49386 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling Testing Data\n",
    "X_test_scaled_standard = pd.DataFrame(standard_scaler.transform(X_test))\n",
    "X_test_scaled_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c34d9",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e89d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PCA to Determine Number of Components For PCA\n",
    "pca_explore = PCA()\n",
    "pca_explore.fit(X_train_scaled_standard)\n",
    "cumsum = np.cumsum(pca_explore.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba9b2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating Number Of Components that Explain 99.9% of Training Data's Variance\n",
    "d = np.argmax(cumsum >= 0.999) + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f66e4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting PCA Dimensions to Explained Variance of Training Data\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 1000, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.999], \"k:\")\n",
    "plt.plot([0, d], [0.999, 0.999], \"k:\")\n",
    "plt.plot(d, 0.999, \"ko\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc547e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=556)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting PCA on Scaled Training Data With 556 Components\n",
    "pca = PCA(n_components = 556)\n",
    "pca.fit(X_train_scaled_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e911ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing Scaled Training Data Using PCA\n",
    "X_train_reduced_PCA = pca.transform(X_train_scaled_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15051b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.477114e-15\n",
       "1     -6.876220e-16\n",
       "2     -1.252682e-15\n",
       "3     -4.329472e-16\n",
       "4     -6.048527e-16\n",
       "           ...     \n",
       "551   -1.766807e-16\n",
       "552   -1.405437e-16\n",
       "553    4.257845e-17\n",
       "554    4.218052e-17\n",
       "555   -1.354950e-16\n",
       "Length: 556, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Means of Scaled Data\n",
    "pd.DataFrame(X_train_reduced_PCA).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a98851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      87.041952\n",
       "1      50.784564\n",
       "2      44.977511\n",
       "3      41.107785\n",
       "4      33.072866\n",
       "         ...    \n",
       "551     5.285713\n",
       "552     5.241031\n",
       "553     5.195313\n",
       "554     5.175973\n",
       "555     5.111665\n",
       "Length: 556, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Deviations of Scaled Data\n",
    "pd.DataFrame(X_train_reduced_PCA).std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7793257",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fe0b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dimension of the original space\n",
    "input_dim = 49386\n",
    "\n",
    "# This is the dimension of the latent space (encoding space). The Same Number of Dimensions as PCA is Used Here\n",
    "latent_dim = 556\n",
    "\n",
    "# Creating the Layers for the Encoder\n",
    "encoder = Sequential([\n",
    "    Dense(2048, activation='sigmoid', input_shape=(input_dim,)),\n",
    "    Dense(latent_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Creating the Layers for the Decoder\n",
    "decoder = Sequential([\n",
    "    Dense(2048, activation='sigmoid', input_shape=(latent_dim,)),\n",
    "    Dense(input_dim, activation=None)\n",
    "])\n",
    "\n",
    "# Sigmoid was used for the activation functions because the relu was zeroing out the encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d03a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Autoencoder Model\n",
    "autoencoder = Model(inputs=encoder.input, outputs=decoder(encoder.output))\n",
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Fitting Autoencoder and Capturing Training History\n",
    "model_history = autoencoder.fit(X_train_scaled_standard, X_train_scaled_standard, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# Using the Autoencoder to Reduce the Training Data\n",
    "X_train_reduced_encoder = encoder.predict(X_train_scaled_standard.to_numpy())\n",
    "\n",
    "# Saving the Reduced Data to a CSV, training is costly enough to warrant saving data\n",
    "pd.DataFrame(X_train_reduced_encoder).to_csv(\"Data/X_train_reduced_encoder.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0d1fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Autoencoder to Reduce the Testing Data. I am reducing and saving the encoded testing data because the autoencoder\n",
    "# takes a really long time to train.\n",
    "X_test_reduced_encoder = encoder.predict(X_test_scaled_standard.to_numpy())\n",
    "\n",
    "# Saving the Reduced Data to a CSV, training is costly enough to warrant saving data\n",
    "pd.DataFrame(X_test_reduced_encoder).to_csv(\"Data/X_test_reduced_encoder.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9ac58e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting AutoEncoder's Learning by Epoch\n",
    "plt.plot(model_history.history[\"loss\"])\n",
    "plt.title(\"Loss vs. Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cecdc",
   "metadata": {},
   "source": [
    "## Locally Linear Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cbdaae3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocallyLinearEmbedding(n_components=556, n_neighbors=100)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting lle model on the scaled training dataset\n",
    "lle = LocallyLinearEmbedding(n_components=556, n_neighbors=100)\n",
    "lle.fit(X_train_scaled_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e84b7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting lle model on the scaled dataset\n",
    "X_train_reduced_lle = lle.transform(X_train_scaled_standard)\n",
    "X_test_reduced_lle = lle.transform(X_test_scaled_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "62902e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.032993</td>\n",
       "      <td>0.075504</td>\n",
       "      <td>-0.024893</td>\n",
       "      <td>0.025266</td>\n",
       "      <td>-0.044524</td>\n",
       "      <td>0.011766</td>\n",
       "      <td>0.091599</td>\n",
       "      <td>-0.031601</td>\n",
       "      <td>0.032080</td>\n",
       "      <td>0.038107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>-0.014187</td>\n",
       "      <td>0.019824</td>\n",
       "      <td>-0.032698</td>\n",
       "      <td>0.008149</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>-0.021932</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.076592</td>\n",
       "      <td>0.057687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.103324</td>\n",
       "      <td>-0.004648</td>\n",
       "      <td>-0.109559</td>\n",
       "      <td>-0.067381</td>\n",
       "      <td>0.059739</td>\n",
       "      <td>-0.066872</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.086816</td>\n",
       "      <td>0.016830</td>\n",
       "      <td>-0.021021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002444</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.006576</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.019071</td>\n",
       "      <td>0.024642</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>-0.020979</td>\n",
       "      <td>0.026369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015930</td>\n",
       "      <td>-0.034330</td>\n",
       "      <td>-0.009101</td>\n",
       "      <td>0.071288</td>\n",
       "      <td>-0.018815</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>-0.025476</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001057</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.009244</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.013734</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>-0.015771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.057535</td>\n",
       "      <td>0.051177</td>\n",
       "      <td>-0.005495</td>\n",
       "      <td>-0.017868</td>\n",
       "      <td>-0.001598</td>\n",
       "      <td>-0.052554</td>\n",
       "      <td>0.026168</td>\n",
       "      <td>-0.012927</td>\n",
       "      <td>-0.008378</td>\n",
       "      <td>-0.029535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.014537</td>\n",
       "      <td>0.021392</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.011386</td>\n",
       "      <td>0.009245</td>\n",
       "      <td>-0.012726</td>\n",
       "      <td>0.019664</td>\n",
       "      <td>0.028015</td>\n",
       "      <td>0.005733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.077987</td>\n",
       "      <td>0.004879</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>-0.036779</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>-0.016105</td>\n",
       "      <td>-0.005840</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>-0.028812</td>\n",
       "      <td>0.028207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044711</td>\n",
       "      <td>0.033574</td>\n",
       "      <td>0.053171</td>\n",
       "      <td>0.026756</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.006615</td>\n",
       "      <td>0.022149</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.006795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>-0.065702</td>\n",
       "      <td>-0.076183</td>\n",
       "      <td>0.013948</td>\n",
       "      <td>-0.077058</td>\n",
       "      <td>-0.019146</td>\n",
       "      <td>-0.040787</td>\n",
       "      <td>-0.046789</td>\n",
       "      <td>0.046299</td>\n",
       "      <td>-0.003696</td>\n",
       "      <td>0.114066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003577</td>\n",
       "      <td>-0.015887</td>\n",
       "      <td>0.026354</td>\n",
       "      <td>-0.015860</td>\n",
       "      <td>0.035535</td>\n",
       "      <td>-0.061822</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>0.019395</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>0.066350</td>\n",
       "      <td>-0.010723</td>\n",
       "      <td>-0.009690</td>\n",
       "      <td>-0.040616</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.009388</td>\n",
       "      <td>-0.052059</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>-0.014984</td>\n",
       "      <td>-0.053367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021917</td>\n",
       "      <td>0.008022</td>\n",
       "      <td>0.007402</td>\n",
       "      <td>0.028272</td>\n",
       "      <td>-0.003423</td>\n",
       "      <td>-0.002930</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>-0.009738</td>\n",
       "      <td>0.037813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>-0.040105</td>\n",
       "      <td>0.025077</td>\n",
       "      <td>0.038192</td>\n",
       "      <td>-0.026515</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>-0.050749</td>\n",
       "      <td>-0.020719</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>-0.032748</td>\n",
       "      <td>0.015739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052961</td>\n",
       "      <td>0.010822</td>\n",
       "      <td>-0.014885</td>\n",
       "      <td>-0.034053</td>\n",
       "      <td>0.032808</td>\n",
       "      <td>0.039263</td>\n",
       "      <td>-0.038304</td>\n",
       "      <td>-0.050800</td>\n",
       "      <td>-0.030165</td>\n",
       "      <td>0.006493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0.015998</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>-0.002884</td>\n",
       "      <td>-0.013044</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>-0.118218</td>\n",
       "      <td>-0.078286</td>\n",
       "      <td>-0.019106</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.022416</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>-0.015030</td>\n",
       "      <td>-0.005329</td>\n",
       "      <td>0.014879</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>-0.001757</td>\n",
       "      <td>-0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0.040118</td>\n",
       "      <td>0.034155</td>\n",
       "      <td>-0.013468</td>\n",
       "      <td>0.055312</td>\n",
       "      <td>0.049253</td>\n",
       "      <td>-0.007656</td>\n",
       "      <td>-0.030943</td>\n",
       "      <td>-0.021309</td>\n",
       "      <td>-0.012026</td>\n",
       "      <td>0.013159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>-0.025498</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>0.023221</td>\n",
       "      <td>0.054018</td>\n",
       "      <td>-0.033740</td>\n",
       "      <td>0.042046</td>\n",
       "      <td>-0.007487</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.006357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 556 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.032993  0.075504 -0.024893  0.025266 -0.044524  0.011766  0.091599   \n",
       "1    0.103324 -0.004648 -0.109559 -0.067381  0.059739 -0.066872  0.007526   \n",
       "2    0.015930 -0.034330 -0.009101  0.071288 -0.018815 -0.040200 -0.025476   \n",
       "3   -0.057535  0.051177 -0.005495 -0.017868 -0.001598 -0.052554  0.026168   \n",
       "4   -0.077987  0.004879  0.007687 -0.036779  0.115077 -0.016105 -0.005840   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "553 -0.065702 -0.076183  0.013948 -0.077058 -0.019146 -0.040787 -0.046789   \n",
       "554  0.066350 -0.010723 -0.009690 -0.040616  0.023858  0.009388 -0.052059   \n",
       "555 -0.040105  0.025077  0.038192 -0.026515  0.016662 -0.050749 -0.020719   \n",
       "556  0.015998  0.014467  0.034200 -0.002884 -0.013044  0.018160 -0.118218   \n",
       "557  0.040118  0.034155 -0.013468  0.055312  0.049253 -0.007656 -0.030943   \n",
       "\n",
       "          7         8         9    ...       546       547       548  \\\n",
       "0   -0.031601  0.032080  0.038107  ...  0.009786 -0.014187  0.019824   \n",
       "1    0.086816  0.016830 -0.021021  ... -0.002444 -0.003357  0.006994   \n",
       "2    0.000920  0.002616  0.005220  ... -0.001057  0.000338 -0.002173   \n",
       "3   -0.012927 -0.008378 -0.029535  ...  0.001392  0.014537  0.021392   \n",
       "4    0.000899 -0.028812  0.028207  ... -0.044711  0.033574  0.053171   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "553  0.046299 -0.003696  0.114066  ... -0.003577 -0.015887  0.026354   \n",
       "554  0.020644 -0.014984 -0.053367  ...  0.021917  0.008022  0.007402   \n",
       "555  0.005706 -0.032748  0.015739  ...  0.052961  0.010822 -0.014885   \n",
       "556 -0.078286 -0.019106  0.023853  ...  0.001691  0.022416  0.005272   \n",
       "557 -0.021309 -0.012026  0.013159  ...  0.004763 -0.025498  0.007938   \n",
       "\n",
       "          549       550       551       552       553       554       555  \n",
       "0   -0.032698  0.008149 -0.005573 -0.021932  0.017084  0.076592  0.057687  \n",
       "1    0.006576  0.012284  0.019071  0.024642  0.003561 -0.020979  0.026369  \n",
       "2   -0.000051 -0.009244  0.003062  0.013734  0.007945  0.003001 -0.015771  \n",
       "3    0.011182  0.011386  0.009245 -0.012726  0.019664  0.028015  0.005733  \n",
       "4    0.026756  0.002055  0.006615  0.022149  0.002981  0.000999  0.006795  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "553 -0.015860  0.035535 -0.061822 -0.008046  0.019395  0.007570  0.002388  \n",
       "554  0.028272 -0.003423 -0.002930  0.000538  0.011062 -0.009738  0.037813  \n",
       "555 -0.034053  0.032808  0.039263 -0.038304 -0.050800 -0.030165  0.006493  \n",
       "556  0.010921 -0.015030 -0.005329  0.014879  0.009372 -0.001757 -0.001489  \n",
       "557  0.023221  0.054018 -0.033740  0.042046 -0.007487  0.003053  0.006357  \n",
       "\n",
       "[558 rows x 556 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_reduced_lle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd330141",
   "metadata": {},
   "source": [
    "## Summary Feature Extraction\n",
    "The result from feature extraction is three datasets containing training data resulting from three different dimensionality reduction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324fc64a",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "110d222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compile model based on different hyperperameters.\n",
    "def build_model(n_hidden=1, n_neurons=30):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(556, ))) # Creating Input Layer\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\")) # Adding Hidden Layers with relu activation.\n",
    "    model.add(tf.keras.layers.Dense(3)) # Final Layer with 3 neurons for three different classes (CN, MCI, AD)\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']) # Compiling Model with Cross Entropy as loss.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43354ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamf\\AppData\\Local\\Temp/ipykernel_4548/126143118.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_classifier_PCA = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)\n"
     ]
    }
   ],
   "source": [
    "# Wrapping keras Multilayer Perceptron in scikit learn wrapper.\n",
    "keras_classifier_PCA = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30d777be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamf\\AppData\\Local\\Temp/ipykernel_4548/4129951741.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_classifier_autoencoder = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)\n"
     ]
    }
   ],
   "source": [
    "# Wrapping keras Multilayer Perceptron in scikit learn wrapper.\n",
    "keras_classifier_autoencoder = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d62f3c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamf\\AppData\\Local\\Temp/ipykernel_4548/4005422230.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_classifier_lle = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)\n"
     ]
    }
   ],
   "source": [
    "# Wrapping keras Multilayer Perceptron in scikit learn wrapper.\n",
    "keras_classifier_lle = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "074b5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perameters for GridSearchCV\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [1, 2, 3, 4],\n",
    "    \"n_neurons\": [64, 128, 256, 384, 512, 640, 786]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b149a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 4.8425 - accuracy: 0.4194\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 5.7652 - accuracy: 0.4462\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 6.4505 - accuracy: 0.4462\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.6839 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.9496 - accuracy: 0.4677\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.6105 - accuracy: 0.4355\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.0401 - accuracy: 0.5806\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.4638 - accuracy: 0.4892\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 5.1796 - accuracy: 0.5215\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.8101 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.1754 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.6627 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 6.0027 - accuracy: 0.4516\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 5.0986 - accuracy: 0.4570\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.1403 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 4.8180 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 4.2278 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 6.1045 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 4.2947 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 4.4352 - accuracy: 0.5699\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 5.8811 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.2044 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.3460 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.9413 - accuracy: 0.4516\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.7040 - accuracy: 0.4785\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.8694 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.0032 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.5929 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.4887 - accuracy: 0.6022\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.4194 - accuracy: 0.5484\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.0219 - accuracy: 0.5430\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.5753 - accuracy: 0.5753\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.4074 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6397 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8692 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0205 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2669 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7027 - accuracy: 0.6022\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2700 - accuracy: 0.5430\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3637 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.5250 - accuracy: 0.5860\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9994 - accuracy: 0.5591\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.5112 - accuracy: 0.4892\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9351 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.4993 - accuracy: 0.5215\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.4271 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9597 - accuracy: 0.4677\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.5806 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.4930 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9406 - accuracy: 0.6452\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.2596 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.5394 - accuracy: 0.5215\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.3293 - accuracy: 0.6075\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9204 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0954 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8635 - accuracy: 0.5806\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.7183 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4491 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.9178 - accuracy: 0.5860\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3686 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6543 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5124 - accuracy: 0.5860\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4779 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.3076 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.7027 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.2055 - accuracy: 0.4570\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.7035 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9422 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.5772 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.4017 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.3314 - accuracy: 0.5806\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.3198 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.1117 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.9015 - accuracy: 0.5484\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.6890 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.5578 - accuracy: 0.5699\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.6071 - accuracy: 0.5753\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1261 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.7956 - accuracy: 0.5484\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.7669 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.8905 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.1102 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.1772 - accuracy: 0.5860\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.6683 - accuracy: 0.5591\n",
      "{'n_hidden': 3, 'n_neurons': 256}\n",
      "0.5698924660682678\n"
     ]
    }
   ],
   "source": [
    "# Fitting GridSearchCV on PCA data\n",
    "PCA_grid_cv = GridSearchCV(keras_classifier_PCA, param_distribs, cv=3, verbose=0)\n",
    "PCA_grid_cv.fit(X_train_reduced_PCA, y_train_num, epochs=50, verbose=0)\n",
    "print(PCA_grid_cv.best_params_)\n",
    "print(PCA_grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "64772cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 1.4334 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.3224 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.4163 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.7058 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.5634 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.6057 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7662 - accuracy: 0.4409\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7339 - accuracy: 0.4892\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7889 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.8141 - accuracy: 0.4462\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.6579 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.8534 - accuracy: 0.5484\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.8420 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7277 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.8655 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.9667 - accuracy: 0.4570\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.6924 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.8586 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.9569 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.8113 - accuracy: 0.4785\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.9664 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.2369 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.2028 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.3263 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.6553 - accuracy: 0.4409\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.2346 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.4471 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.7886 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.3786 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.7657 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.9515 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.5133 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.8298 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9403 - accuracy: 0.4624\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.7682 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2491 - accuracy: 0.5215\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3205 - accuracy: 0.4516\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9642 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0315 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9049 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0036 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3106 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.7235 - accuracy: 0.4785\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.5000 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.0534 - accuracy: 0.5430\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.9704 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.9464 - accuracy: 0.5000\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.2996 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.1690 - accuracy: 0.4785\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.3321 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.8763 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.7210 - accuracy: 0.4892\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.1127 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.7795 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4839 - accuracy: 0.4677\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0394 - accuracy: 0.4785\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9683 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.8411 - accuracy: 0.4839\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1804 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0995 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.1291 - accuracy: 0.4785\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.4931 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0769 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.3502 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.3585 - accuracy: 0.4677\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.5686 - accuracy: 0.4892\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.4859 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.9959 - accuracy: 0.5699\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.8861 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.0701 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 4.2868 - accuracy: 0.5215\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.9666 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.9868 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.1999 - accuracy: 0.4516\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 4.4729 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.0088 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.7440 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3176 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.2831 - accuracy: 0.5215\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.2744 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.2948 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.6639 - accuracy: 0.4785\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.9716 - accuracy: 0.4785\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5.2712 - accuracy: 0.5054\n",
      "{'n_hidden': 4, 'n_neurons': 128}\n",
      "0.5340501666069031\n"
     ]
    }
   ],
   "source": [
    "# Fitting GridSearchCV on autoencoder data\n",
    "autoencoder_grid_cv = GridSearchCV(keras_classifier_autoencoder, param_distribs, cv=3, verbose=0)\n",
    "autoencoder_grid_cv.fit(X_train_reduced_encoder, y_train_num, epochs=50, verbose = 0)\n",
    "print(autoencoder_grid_cv.best_params_)\n",
    "print(autoencoder_grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3916abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 1.0039 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8115 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9737 - accuracy: 0.5699\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.0363 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8540 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0023 - accuracy: 0.5914\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.0783 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8749 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.0546 - accuracy: 0.5860\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.1299 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8906 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0843 - accuracy: 0.5914\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.1594 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.9170 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0861 - accuracy: 0.5914\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.1896 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.9032 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.1207 - accuracy: 0.5914\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.1598 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.9390 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.1297 - accuracy: 0.5968\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.3794 - accuracy: 0.5484\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9817 - accuracy: 0.5968\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.2462 - accuracy: 0.5591\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.4045 - accuracy: 0.5591\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.1054 - accuracy: 0.6129\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.3144 - accuracy: 0.5860\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.4822 - accuracy: 0.5591\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.2072 - accuracy: 0.6022\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.3552 - accuracy: 0.5591\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.5255 - accuracy: 0.5591\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.1802 - accuracy: 0.6344\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.4295 - accuracy: 0.5753\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5326 - accuracy: 0.5753\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.1277 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.4021 - accuracy: 0.5860\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5648 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2091 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5052 - accuracy: 0.5860\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6401 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2202 - accuracy: 0.6290\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.4877 - accuracy: 0.5914\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7952 - accuracy: 0.5753\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.3589 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.6249 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.6809 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.4614 - accuracy: 0.5699\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7032 - accuracy: 0.5591\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.7195 - accuracy: 0.5806\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.5483 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7828 - accuracy: 0.5699\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.6732 - accuracy: 0.5430\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.5166 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.6706 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6824 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2247 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6802 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.5396 - accuracy: 0.4301\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.3803 - accuracy: 0.6237\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.5429 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7555 - accuracy: 0.5323\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2282 - accuracy: 0.5484\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.5108 - accuracy: 0.4677\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.8422 - accuracy: 0.5968\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.5938 - accuracy: 0.5430\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.0596 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.9257 - accuracy: 0.5054\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.8575 - accuracy: 0.5645\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.8695 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.2003 - accuracy: 0.4946\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.9280 - accuracy: 0.4570\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.8816 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4202 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.9771 - accuracy: 0.4624\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.6968 - accuracy: 0.5161\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2493 - accuracy: 0.4731\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9326 - accuracy: 0.5108\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.4066 - accuracy: 0.4032\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.5538 - accuracy: 0.5269\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.7979 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 3.1844 - accuracy: 0.5538\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.9425 - accuracy: 0.5376\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.4403 - accuracy: 0.4516\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.9254 - accuracy: 0.4839\n",
      "{'n_hidden': 1, 'n_neurons': 786}\n",
      "0.5949820876121521\n"
     ]
    }
   ],
   "source": [
    "# Fitting GridSearchCV on lle data\n",
    "lle_grid_cv = GridSearchCV(keras_classifier_lle, param_distribs, cv=3, verbose=0)\n",
    "lle_grid_cv.fit(X_train_reduced_lle, y_train_num, epochs=50, verbose=0)\n",
    "print(lle_grid_cv.best_params_)\n",
    "print(lle_grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7bb9854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamf\\AppData\\Local\\Temp/ipykernel_4548/2475719743.py:12: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  best_mlp = tf.keras.wrappers.scikit_learn.KerasClassifier(build_best_model)\n"
     ]
    }
   ],
   "source": [
    "def build_best_model(n_hidden=1, n_neurons=786):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(556, ))) # Creating Input Layer\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\")) # Adding Hidden Layers with relu activation.\n",
    "    model.add(tf.keras.layers.Dense(3)) # Final Layer with 3 neurons for three different classes (CN, MCI, AD)\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']) # Compiling Model with Cross Entropy as loss.\n",
    "    return model\n",
    "\n",
    "best_mlp = tf.keras.wrappers.scikit_learn.KerasClassifier(build_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "872f4645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a12abccca0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mlp.fit(X_train_reduced_lle, y_train_num, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff1c4f",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa3ddeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perameters for GridSearchCV\n",
    "param_distribs = {\n",
    "    \"max_leaf_nodes\": [5, 10, 25, 50, 75, 150, 200],\n",
    "    \"max_depth\": [5, 10, 25, 50, 75, 150, 200],\n",
    "    \"n_estimators\": [50, 100, 150, 200]\n",
    "}\n",
    "\n",
    "# Creating grid search object with f1_score as scoring function\n",
    "pca_grid_rf = GridSearchCV(RandomForestClassifier(), param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)\n",
    "autoencoder_grid_rf = GridSearchCV(RandomForestClassifier(), param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)\n",
    "lle_grid_rf = GridSearchCV(RandomForestClassifier(), param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "656f62f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 196 candidates, totalling 588 fits\n",
      "{'max_depth': 25, 'max_leaf_nodes': 75, 'n_estimators': 50}\n",
      "0.49449914182566873\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for pca\n",
    "pca_grid_rf.fit(X_train_reduced_PCA, y_train)\n",
    "print(pca_grid_rf.best_params_)\n",
    "print(pca_grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "737200a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 196 candidates, totalling 588 fits\n",
      "{'max_depth': 150, 'max_leaf_nodes': 75, 'n_estimators': 200}\n",
      "0.5071805880099025\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for the autoencoder\n",
    "autoencoder_grid_rf.fit(X_train_reduced_encoder, y_train)\n",
    "print(autoencoder_grid_rf.best_params_)\n",
    "print(autoencoder_grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "45f1d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 196 candidates, totalling 588 fits\n",
      "{'max_depth': 150, 'max_leaf_nodes': 200, 'n_estimators': 50}\n",
      "0.48042920947428946\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for lle\n",
    "lle_grid_rf.fit(X_train_reduced_lle, y_train)\n",
    "print(lle_grid_rf.best_params_)\n",
    "print(lle_grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0664b392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=150, max_leaf_nodes=75, n_estimators=200)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf = RandomForestClassifier(max_depth=150, max_leaf_nodes=75, n_estimators=200)\n",
    "best_rf.fit(X_train_reduced_encoder, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13080746",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d05da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perameters for GridSearchCV\n",
    "param_distribs = {\n",
    "    \"max_leaf_nodes\": [5, 10, 25, 50, 75, 150, 200],\n",
    "    \"max_depth\": [5, 10, 25, 50, 75, 150, 200],\n",
    "    \"n_estimators\": [50, 100, 150, 200]\n",
    "}\n",
    "\n",
    "# Creating grid search object with f1_score as scoring function\n",
    "pca_grid_extree = GridSearchCV(ExtraTreesClassifier(), param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)\n",
    "autoencoder_grid_extree = GridSearchCV(ExtraTreesClassifier(), param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)\n",
    "lle_grid_extree = GridSearchCV(ExtraTreesClassifier(), param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ffa4b787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 196 candidates, totalling 588 fits\n",
      "{'max_depth': 200, 'max_leaf_nodes': 150, 'n_estimators': 50}\n",
      "0.4842771599543691\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for pca\n",
    "pca_grid_extree.fit(X_train_reduced_PCA, y_train)\n",
    "print(pca_grid_extree.best_params_)\n",
    "print(pca_grid_extree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26ed3d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 196 candidates, totalling 588 fits\n",
      "{'max_depth': 75, 'max_leaf_nodes': 200, 'n_estimators': 50}\n",
      "0.5202546097424787\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for the autoencoder\n",
    "autoencoder_grid_extree.fit(X_train_reduced_encoder, y_train)\n",
    "print(autoencoder_grid_extree.best_params_)\n",
    "print(autoencoder_grid_extree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d63c2470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 196 candidates, totalling 588 fits\n",
      "{'max_depth': 25, 'max_leaf_nodes': 150, 'n_estimators': 50}\n",
      "0.47156471836859654\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for lle\n",
    "lle_grid_extree.fit(X_train_reduced_lle, y_train)\n",
    "print(lle_grid_extree.best_params_)\n",
    "print(lle_grid_extree.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "083b272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(max_depth=75, max_leaf_nodes=200, n_estimators=50)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_extree = ExtraTreesClassifier(max_depth=75, max_leaf_nodes=200, n_estimators=50)\n",
    "best_extree.fit(X_train_reduced_encoder, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7440c4",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2583a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "    \"C\": [0.01, 0.1, 1, 5, 10],\n",
    "    \"kernel\": [\"poly\",\"rbf\"],\n",
    "    \"degree\":[3, 8, 13, 20]\n",
    "}\n",
    "\n",
    "# Creating grid search objects with f1_score as scoring function\n",
    "pca_grid_SVC = GridSearchCV(SVC(), param_grid=param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)\n",
    "autoencoder_grid_SVC = GridSearchCV(SVC(), param_grid=param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)\n",
    "lle_grid_SVC = GridSearchCV(SVC(), param_grid=param_distribs, scoring='f1_weighted', cv=3, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ad571606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "{'C': 5, 'degree': 3, 'kernel': 'rbf'}\n",
      "0.48436873539633013\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for pca\n",
    "pca_grid_SVC.fit(X_train_reduced_PCA, y_train)\n",
    "print(pca_grid_SVC.best_params_)\n",
    "print(pca_grid_SVC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "41c98e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "{'C': 0.01, 'degree': 13, 'kernel': 'poly'}\n",
      "0.4920541541645919\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for the autoencoder\n",
    "autoencoder_grid_SVC.fit(X_train_reduced_encoder, y_train)\n",
    "print(autoencoder_grid_SVC.best_params_)\n",
    "print(autoencoder_grid_SVC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97d18379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "{'C': 0.01, 'degree': 3, 'kernel': 'poly'}\n",
      "0.44172618366166755\n"
     ]
    }
   ],
   "source": [
    "# Fitting grid search for lle\n",
    "lle_grid_SVC.fit(X_train_reduced_lle, y_train)\n",
    "print(lle_grid_SVC.best_params_)\n",
    "print(lle_grid_SVC.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "46dde92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, degree=13, kernel='poly')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svc = SVC(C=0.1, degree=13, kernel='poly')\n",
    "best_svc.fit(X_train_reduced_encoder, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a93b0",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "881db43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamf\\AppData\\Local\\Temp/ipykernel_4548/2207224872.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_mlp = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_best_model)\n"
     ]
    }
   ],
   "source": [
    "# Creating keras mlp object for voting classifier.\n",
    "keras_mlp = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_best_model)\n",
    "\n",
    "# Creating ensemble model with soft voting with MLPClassifier(), RandomForestClassifier(), and ExtraTreesClassifier()\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[('rf', RandomForestClassifier(max_depth=150, max_leaf_nodes=75, n_estimators=200)),\n",
    "                ('extree', ExtraTreesClassifier(max_depth=75, max_leaf_nodes=200, n_estimators=50)),\n",
    "                ('svc', SVC(C=0.1, degree=13, kernel='poly', probability=True))],\n",
    "    voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c553f266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf',\n",
       "                              RandomForestClassifier(max_depth=150,\n",
       "                                                     max_leaf_nodes=75,\n",
       "                                                     n_estimators=200)),\n",
       "                             ('extree',\n",
       "                              ExtraTreesClassifier(max_depth=75,\n",
       "                                                   max_leaf_nodes=200,\n",
       "                                                   n_estimators=50)),\n",
       "                             ('svc',\n",
       "                              SVC(C=0.1, degree=13, kernel='poly',\n",
       "                                  probability=True))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting ensemble model\n",
    "voting_clf_soft.fit(X_train_reduced_encoder, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f027f2bd",
   "metadata": {},
   "source": [
    "## Comparing Models on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e6c51808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score: 0.2699947589098533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.10769231, 0.8       ])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_mlp.predict(X_test_reduced_lle)\n",
    "\n",
    "print(\"f1 Score:\", f1_score(y_test_num, y_pred, average='macro'))\n",
    "\n",
    "matrix = confusion_matrix(y_test_num, y_pred)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "38de4306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score: 0.2982456140350877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.15384615, 0.82727273])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_rf.predict(X_test_reduced_encoder)\n",
    "\n",
    "print(\"f1 Score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2bf11b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score: 0.30867433619727197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.23076923, 0.74545455])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_extree.predict(X_test_reduced_encoder)\n",
    "\n",
    "print(\"f1 Score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b151a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score: 0.2997444322745528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.23076923, 0.70909091])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_svc.predict(X_test_reduced_encoder)\n",
    "\n",
    "print(\"f1 Score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "16576dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score: 0.26038622428514124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.06153846, 0.86363636])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = voting_clf_soft.predict(X_test_reduced_encoder)\n",
    "\n",
    "print(\"f1 Score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4145cc",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://www.tensorflow.org/tutorials/keras/classification\n",
    "2. https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb\n",
    "3. https://ekamperi.github.io/machine%20learning/2021/01/21/encoder-decoder-model.html\n",
    "4. https://pbpython.com/categorical-encoding.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
